# 高级多领域 OpenAlex 爬虫

这是一个功能强大的多领域学术论文爬虫，专门设计用于从 OpenAlex 数据库爬取各个学科领域的论文信息。

## 🌟 主要特性

- **多领域覆盖**: 涵盖14个主要学科领域，包括AI、计算机科学、电子信息、文学、教育等
- **异步高效**: 使用 asyncio 和 aiohttp 实现高并发爬取
- **智能调度**: 优先爬取AI/计算机领域，同时保证其他领域的均衡覆盖
- **进度保存**: 支持断点续传，可随时停止和恢复
- **大小控制**: 自动控制数据库大小在50GB以下
- **实时监控**: 提供详细的进度报告和统计信息

## 📊 覆盖领域

| 领域 | 权重 | 最大论文数 | 主要关键词 |
|------|------|------------|------------|
| 人工智能 | 3.0 | 500,000 | AI, 机器学习, 深度学习, 神经网络 |
| 计算机科学 | 2.5 | 400,000 | 算法, 数据结构, 软件工程 |
| 电子信息 | 2.0 | 300,000 | 信号处理, 通信, 嵌入式系统 |
| 数学 | 1.5 | 250,000 | 统计学, 概率论, 线性代数 |
| 物理学 | 1.5 | 200,000 | 量子力学, 热力学, 光学 |
| 生物学 | 1.5 | 200,000 | 遗传学, 分子生物学, 生物信息学 |
| 化学 | 1.5 | 150,000 | 有机化学, 无机化学, 催化 |
| 医学 | 1.5 | 200,000 | 临床研究, 药理学, 免疫学 |
| 文学 | 1.0 | 100,000 | 文学批评, 比较文学, 创意写作 |
| 教育学 | 1.0 | 100,000 | 教育研究, 教学法, 课程设计 |
| 语言学 | 1.0 | 80,000 | 语言习得, 句法学, 语义学 |
| 哲学 | 1.0 | 60,000 | 伦理学, 认识论, 形而上学 |
| 心理学 | 1.2 | 100,000 | 认知心理学, 社会心理学 |
| 经济学 | 1.2 | 100,000 | 微观经济学, 宏观经济学 |

## 🚀 快速开始

### 1. 安装依赖

```bash
pip install aiohttp
```

### 2. 运行爬虫

```bash
# 使用默认设置运行
python run_advanced_crawler.py

# 指定数据库文件
python run_advanced_crawler.py --db my_papers.db

# 设置并发数
python run_advanced_crawler.py --concurrent 5

# 查看当前状态
python run_advanced_crawler.py --status
```

### 3. 监控进度

爬虫运行时会实时显示：
- 总论文数和数据库大小
- 各领域爬取进度
- 运行时间和平均速度
- 完成率统计

## 📁 文件结构

```
crawler/
├── advanced_crawler.py      # 核心爬虫代码
├── run_advanced_crawler.py  # 运行脚本
├── requirements_advanced.txt # 依赖包列表
├── README_advanced.md       # 说明文档
├── crawl_progress.json      # 进度文件（自动生成）
├── crawler.log             # 日志文件（自动生成）
└── openalex_advanced.db    # 数据库文件（自动生成）
```

## 🛠️ 高级用法

### 自定义配置

可以修改 `advanced_crawler.py` 中的 `DOMAIN_CONFIG` 来调整：
- 各领域的搜索关键词
- 权重和最大论文数
- 年份范围

### 断点续传

爬虫会自动保存进度到 `crawl_progress.json`，包括：
- 每个任务的当前游标位置
- 已爬取的论文数量
- 完成状态

### 安全停止

按 `Ctrl+C` 可以安全停止爬虫，进度会被保存。

## 📈 性能优化

- **并发控制**: 默认3个并发连接，可根据网络情况调整
- **批量保存**: 每50篇论文批量保存到数据库
- **智能限流**: 自动处理API限流和重试
- **内存优化**: 使用流式处理，避免内存溢出

## 🔧 故障排除

### 常见问题

1. **网络连接问题**
   - 检查网络连接
   - 调整并发数（降低到1-2）

2. **数据库锁定**
   - 确保没有其他程序在使用数据库
   - 重启爬虫

3. **API限流**
   - 爬虫会自动处理，等待时间会显示在日志中
   - 可以降低并发数

### 日志查看

```bash
tail -f crawler.log
```

## 📊 预期结果

- **总论文数**: 约2,500,000-3,000,000篇
- **数据库大小**: 约45-50GB
- **运行时间**: 根据网络情况，约1-2周
- **覆盖范围**: 14个主要学科领域

## ⚠️ 注意事项

1. **网络要求**: 需要稳定的网络连接
2. **存储空间**: 确保有足够的磁盘空间（至少60GB）
3. **API限制**: 遵守OpenAlex的使用条款
4. **数据用途**: 仅用于学术研究，不得商业使用

## 📞 技术支持

如有问题，请检查：
1. 日志文件 `crawler.log`
2. 进度文件 `crawl_progress.json`
3. 数据库文件是否正常

---

**祝您爬取愉快！** 🎉
